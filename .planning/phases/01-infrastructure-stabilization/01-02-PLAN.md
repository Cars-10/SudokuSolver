---
phase: 01-infrastructure-stabilization
plan: 02
type: execute
---

<objective>
Test cross-platform shell script compatibility and verify Docker container runtime with full metrics.

Purpose: Ensure benchmarking infrastructure works consistently on both macOS (development) and Linux (Docker/CI), with all expanded metrics captured correctly.
Output: Verified cross-platform scripts, working Docker container serving benchmarks.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/codebase/STACK.md

**Prior plan summary (load after 01-01 complete):**
@.planning/phases/01-infrastructure-stabilization/01-01-SUMMARY.md

**Relevant source files:**
@Languages/common.sh
@Languages/C/runMe.sh
@Languages/C/setupAndRunMe.sh
@runMeGlobal.sh
@runBenchmarks.sh
@docker-compose.yml

**Prior context from Plan 01:**
- Docker image verified/rebuilt
- common.sh expanded to capture 10 metrics
- Ready for runtime testing
</context>

<tasks>

<task type="auto">
  <name>Task 1: Test Cross-Platform Shell Scripts on macOS</name>
  <files>Languages/common.sh, Languages/C/metrics.json</files>
  <action>
    1. Verify macOS tooling prerequisites:
       - Check gtime installed: `which gtime` (install via `brew install gnu-time` if missing)
       - Check gtimeout installed: `which gtimeout` (install via `brew install coreutils` if missing)

    2. Test common.sh environment detection:
       - Source common.sh and verify TIME_CMD is set to "gtime"
       - Verify TIMEOUT_CMD is set to "gtimeout"

    3. Run C benchmark as the reference test (C is the canonical implementation):
       - `cd Languages/C && ./setupAndRunMe.sh ../../Matrices/1.matrix`
       - Verify exit code is 0

    4. Validate metrics.json output contains all 10 expanded fields:
       - Parse Languages/C/metrics.json
       - Confirm presence of: time, memory, cpu_user, cpu_sys, iterations, status
       - Confirm presence of NEW fields: page_faults_major, page_faults_minor, context_switches_voluntary, context_switches_involuntary, io_inputs, io_outputs
       - Verify iterations = 656 for Matrix 1 (the canonical count)

    5. Test runBenchmarks.sh status command:
       - `./runBenchmarks.sh --status` should list languages without errors

    **What to avoid:**
    - Don't run benchmarks on all languages - just C for verification
    - Don't modify C solver code - it's the reference implementation
  </action>
  <verify>
    - `which gtime` returns path
    - `which gtimeout` returns path
    - `Languages/C/metrics.json` exists and contains valid JSON
    - `jq '.[] | .results[0].iterations' Languages/C/metrics.json` returns 656
    - `jq '.[] | .results[0] | has("page_faults_major")' Languages/C/metrics.json` returns true
  </verify>
  <done>Shell scripts work on macOS, C benchmark produces 656 iterations with all 10 metrics captured</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Cross-platform infrastructure with expanded metrics capture:
    - Docker image built and verified
    - common.sh captures 10 performance metrics
    - Shell scripts tested on macOS with C reference implementation
  </what-built>
  <how-to-verify>
    1. Start the Docker container:
       ```
       cd /Users/vibe/ClaudeCode/SudokuSolver
       docker-compose up -d
       ```

    2. Verify server is running:
       - Visit http://localhost:9001 in browser
       - Confirm the benchmark UI loads (Matrix Runner page)

    3. Run a benchmark inside Docker:
       ```
       docker-compose exec app bash -c "cd /app/Languages/C && ./setupAndRunMe.sh ../../Matrices/1.matrix"
       ```

    4. Verify metrics inside Docker:
       ```
       docker-compose exec app cat /app/Languages/C/metrics.json | jq '.[] | .results[0]'
       ```
       - Confirm iterations = 656
       - Confirm all 10 metric fields are present

    5. (Optional) Test via UI:
       - In browser at http://localhost:9001, select "C" language and Matrix "1"
       - Click Run and verify it completes

    6. Stop container when done:
       ```
       docker-compose down
       ```
  </how-to-verify>
  <resume-signal>Type "approved" if server loads and benchmarks run correctly with all metrics, or describe any issues</resume-signal>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] macOS: gtime and gtimeout available
- [ ] macOS: C benchmark produces 656 iterations
- [ ] macOS: All 10 metrics captured in metrics.json
- [ ] Docker: Container starts via docker-compose
- [ ] Docker: Server accessible at http://localhost:9001
- [ ] Docker: C benchmark runs inside container with correct metrics
</verification>

<success_criteria>
- Cross-platform compatibility verified (macOS development + Linux Docker)
- All 10 performance metrics captured consistently
- Iterations count matches reference (656 for Matrix 1)
- Docker container serves benchmark UI
- Phase 1 complete, ready for Phase 2 (Algorithmic Audit)
</success_criteria>

<output>
After completion, create `.planning/phases/01-infrastructure-stabilization/01-02-SUMMARY.md`:

# Phase 1 Plan 02: Shell Testing + Verification Summary

**[One-liner: what shipped]**

## Accomplishments
- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified
- `path/to/file` - Description

## Decisions Made
[Key decisions and rationale, or "None"]

## Issues Encountered
[Problems and resolutions, or "None"]

## Next Step
Phase 1 complete. Ready for Phase 2: Algorithmic Audit
</output>
