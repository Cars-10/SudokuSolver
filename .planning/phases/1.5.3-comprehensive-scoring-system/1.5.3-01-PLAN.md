---
phase: 1.5.3-comprehensive-scoring-system
plan: 01
type: execute
---

<objective>
Implement comprehensive scoring calculation and update the Scoring Methodology modal.

Purpose: Replace the existing simple weighted formula with a more robust scoring system that better represents language performance across all collected metrics.

Output: Updated scoring calculation in HTMLGenerator.ts and enhanced Scoring Methodology modal explaining the chosen method.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

**Current Implementation:**
@Metrics/HTMLGenerator.ts (lines 1509-1518: current scoring formula)

**Current Scoring Formula:**
```javascript
// Score = (3*CpuRatio + 2*MemRatio + 1*TimeRatio) / 6
const normalizedScore = (3.0 * cpuRatio + 2.0 * memRatio + 1.0 * timeRatio) / 6.0;
```
Where ratios are relative to C baseline (e.g., timeRatio = totalTime / cTotalTime)

**Available Metrics:**
- `time_1` through `time_5`: Execution time per matrix (seconds)
- `memory_1` through `memory_5`: Memory usage per matrix (bytes)
- `iterations_1` through `iterations_5`: Algorithm iterations per matrix
- `cpu_user`, `cpu_sys`: CPU time measurements
- `validated`: Boolean validation status
- `compiler_version`: Toolchain info

**Methodology Modal Location:**
@Metrics/HTMLGenerator.ts (lines 983-1025: methodModal HTML)
</context>

<tasks>

<task type="checkpoint:decision" gate="blocking">
  <decision>Select scoring methodology for the benchmark</decision>
  <context>
    The current formula is a simple weighted composite: Ψ = (3·ρ_cpu + 2·ρ_mem + 1·ρ_time) / 6

    We want to implement a more comprehensive scoring system that better represents overall performance. Consider which method best serves users comparing language implementations.
  </context>
  <options>
    <option id="weighted">
      <name>Weighted Multi-Criteria (Enhanced Current)</name>
      <pros>
        - Already partially implemented, minimal changes needed
        - User-adjustable weights possible (Time: 40%, Memory: 30%, etc.)
        - Easy to understand: higher weight = more importance
        - Can add iteration efficiency to existing formula
      </pros>
      <cons>
        - Weight selection is somewhat arbitrary
        - One metric can still dominate if weights are unbalanced
      </cons>
    </option>
    <option id="geometric">
      <name>Geometric Mean (Industry Standard)</name>
      <pros>
        - Industry standard (SPEC benchmarks, Geekbench)
        - Prevents single metric from dominating
        - Penalizes imbalance (bad in one = lower overall)
        - Mathematically elegant
      </pros>
      <cons>
        - Less intuitive for non-technical users
        - Zero values cause problems (need floor)
        - Harder to explain in modal
      </cons>
    </option>
    <option id="percentile">
      <name>Percentile Ranking</name>
      <pros>
        - Very intuitive: "Faster than 85% of languages"
        - Resistant to outliers
        - Always 0-100 scale
        - Easy comparison across metrics
      </pros>
      <cons>
        - Requires recalculation when new languages added
        - Doesn't show absolute performance
        - Two similar performers could have same rank
      </cons>
    </option>
    <option id="tiered">
      <name>Tier Classification (A/B/C/D/F)</name>
      <pros>
        - Immediately understandable (familiar grading)
        - Quick visual assessment
        - Groups similar performers together
        - Can combine with numeric score
      </pros>
      <cons>
        - Loses granularity (A+ vs A- not distinguished)
        - Boundary effects (just missed A = B)
        - Based on standard deviations, can be confusing
      </cons>
    </option>
    <option id="hybrid">
      <name>Hybrid: Weighted Score + Tier Badge</name>
      <pros>
        - Best of both: numeric precision + visual tier
        - Weighted score for sorting/comparison
        - Tier badge for quick assessment
        - Can show "A (0.85)" format
      </pros>
      <cons>
        - More complex to implement
        - Two systems to explain in modal
        - May be visually busy
      </cons>
    </option>
  </options>
  <resume-signal>Select: weighted, geometric, percentile, tiered, or hybrid</resume-signal>
</task>

<task type="auto">
  <name>Task 2: Implement chosen scoring system and update modal</name>
  <files>Metrics/HTMLGenerator.ts</files>
  <action>
Based on the chosen scoring methodology:

1. **Update the scoring calculation (around line 1509-1518):**
   - Implement the chosen formula
   - Keep C as the baseline reference
   - Handle edge cases (zero values, missing data)
   - Store score breakdown data for tooltip display

2. **Update the Scoring Methodology modal (lines 983-1025):**
   - Replace current formula display with chosen method
   - Explain the methodology clearly
   - Show formula in readable format
   - Include interpretation guide:
     - What scores mean (good vs bad)
     - How to compare languages
     - What metrics contribute

3. **Ensure backward compatibility:**
   - Keep existing data attributes structure
   - Maintain sortRows('score', this) functionality
   - Preserve tooltip score breakdown

4. **Formula implementations by choice:**

   **If weighted:**
   - Keep current structure but add iterations
   - Formula: Ψ = (w₁·ρ_time + w₂·ρ_mem + w₃·ρ_iter + w₄·ρ_cpu) / Σw
   - Default weights: Time=40%, Memory=30%, Iterations=20%, CPU=10%

   **If geometric:**
   - Formula: Ψ = (ρ_time · ρ_mem · ρ_iter · ρ_cpu)^(1/4)
   - Floor values at 0.001 to avoid zero issues
   - Result: <1 is better than C, >1 is worse

   **If percentile:**
   - Calculate percentile rank for each metric
   - Average percentiles: Ψ = avg(pct_time, pct_mem, pct_iter, pct_cpu)
   - Result: 0-100 scale, higher = faster than more languages

   **If tiered:**
   - Calculate standard deviation across all languages
   - A: >1σ above mean, B: 0-1σ above, C: 0-1σ below, D: <1σ below
   - Display as letter grade with color coding

   **If hybrid:**
   - Use weighted score as primary
   - Add tier badge based on score ranges
   - Display as "A (0.85)" or similar
  </action>
  <verify>
- `npm run build` in Metrics/ succeeds without errors
- Score column still renders in HTML output
- Modal displays updated methodology explanation
- No TypeScript errors
  </verify>
  <done>
- Scoring calculation updated with chosen methodology
- Modal explains the methodology clearly
- Score displays correctly in table
- Tooltip shows score breakdown
- Build passes without errors
  </done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] Score calculation uses chosen methodology
- [ ] All rows have valid scores (no NaN, no undefined)
- [ ] Methodology modal explains the formula clearly
- [ ] Scores sort correctly (ascending/descending)
- [ ] Score breakdown tooltip shows contributing factors
- [ ] Build passes: `npm run build` in Metrics/
</verification>

<success_criteria>
- Scoring methodology implemented and working
- Methodology modal updated with clear explanation
- Users can understand what the score means
- No regression in existing functionality
- Phase 1.5.3 complete
</success_criteria>

<output>
After completion, create `.planning/phases/1.5.3-comprehensive-scoring-system/1.5.3-01-SUMMARY.md`
</output>
