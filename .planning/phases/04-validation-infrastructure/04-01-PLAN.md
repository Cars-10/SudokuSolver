---
phase: 04-validation-infrastructure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - Algorithms/common.sh
autonomous: true

must_haves:
  truths:
    - "Benchmark run validates iteration count against C reference during execution"
    - "Benchmark run validates solution satisfies Sudoku constraints during execution"
    - "Validation failure stops benchmark immediately with error code"
    - "Validation failures are logged to benchmark_issues.json with severity"
    - "BruteForce requires exact iteration match; DLX/CP allow +/-1 tolerance"
    - "Temp files are cleaned up before exit on validation failure"
  artifacts:
    - path: "Algorithms/common.sh"
      provides: "Validation functions and run_matrix integration"
      exports: ["detect_algorithm_type", "get_reference_iterations", "validate_iteration_count", "validate_solution", "write_validation_failure"]
  key_links:
    - from: "Algorithms/common.sh:run_matrix()"
      to: "validate_solution() and validate_iteration_count()"
      via: "function calls after extract_iterations, before status determination"
      pattern: "validate_solution.*validate_iteration_count"
    - from: "Algorithms/common.sh:validate_iteration_count()"
      to: "get_reference_iterations()"
      via: "reference lookup by algorithm type and matrix"
      pattern: "get_reference_iterations"
    - from: "Algorithms/common.sh:write_validation_failure()"
      to: "benchmark_issues.json"
      via: "Python JSON append with severity field"
      pattern: "benchmark_issues\\.json.*severity"
---

<objective>
Add validation infrastructure to common.sh that executes during benchmark runs, validating iteration counts against C reference baselines and solution correctness against Sudoku constraints, with fail-fast error handling and benchmark_issues.json logging.

Purpose: Establish algorithmic correctness foundation so invalid implementations are caught immediately during benchmark execution, not discovered later during report analysis.

Output: Modified common.sh with validation functions integrated into run_matrix(), benchmark_issues.json populated with any validation failures.

Note: This phase implements VAL-01, VAL-02, VAL-03, VAL-06 (backend validation). Requirements VAL-04 (visual warning badges) and VAL-05 (diagnostics modal details) are UI-layer concerns explicitly deferred to Phase 6 (Visualization & UI) per user decision in 04-CONTEXT.md: "NO new UI elements in reportâ€”validation results only appear in benchmark_issues.json log".
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase-specific context
@.planning/phases/04-validation-infrastructure/04-CONTEXT.md
@.planning/phases/04-validation-infrastructure/04-RESEARCH.md

# Target file
@Algorithms/common.sh

# Reference data
@Metrics/C_Baselines.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1a: Add algorithm detection and reference lookup functions</name>
  <files>Algorithms/common.sh</files>
  <action>
Add the following functions to common.sh AFTER the OUTPUT PARSING section (after line 119) and BEFORE the MATRIX EXECUTION section. Add a new section header: `# VALIDATION FUNCTIONS`

1. **detect_algorithm_type()** - Parse current working directory to determine algorithm type
   ```bash
   detect_algorithm_type() {
       local cwd=$(pwd)
       if [[ "$cwd" == */Algorithms/BruteForce/* ]]; then
           echo "BruteForce"
       elif [[ "$cwd" == */Algorithms/DLX/* ]]; then
           echo "DLX"
       elif [[ "$cwd" == */Algorithms/CP/* ]]; then
           echo "CP"
       else
           echo "Unknown"
       fi
   }
   ```

2. **get_reference_iterations()** - Look up C reference iteration count
   - Takes matrix_num as argument
   - Uses detect_algorithm_type() to determine baseline set
   - Returns iteration count from C_Baselines.ts values (hardcoded in bash case statement)
   - Returns 0 if unknown algorithm/matrix (skip validation)

   Reference values from C_Baselines.ts:
   - BruteForce: 1=656, 2=439269, 3=98847, 4=9085, 5=445778, 6=622577597
   - DLX: 1=43, 2=111, 3=131, 4=70, 5=1472, 6=65
   - CP: 1=67, 2=87180, 3=4241, 4=1787, 5=31430, 6=69497705

   ```bash
   get_reference_iterations() {
       local matrix_num="$1"
       local algo_type=$(detect_algorithm_type)

       case "$algo_type" in
           BruteForce)
               case "$matrix_num" in
                   1) echo 656 ;;
                   2) echo 439269 ;;
                   3) echo 98847 ;;
                   4) echo 9085 ;;
                   5) echo 445778 ;;
                   6) echo 622577597 ;;
                   *) echo 0 ;;
               esac
               ;;
           DLX)
               case "$matrix_num" in
                   1) echo 43 ;;
                   2) echo 111 ;;
                   3) echo 131 ;;
                   4) echo 70 ;;
                   5) echo 1472 ;;
                   6) echo 65 ;;
                   *) echo 0 ;;
               esac
               ;;
           CP)
               case "$matrix_num" in
                   1) echo 67 ;;
                   2) echo 87180 ;;
                   3) echo 4241 ;;
                   4) echo 1787 ;;
                   5) echo 31430 ;;
                   6) echo 69497705 ;;
                   *) echo 0 ;;
               esac
               ;;
           *)
               echo 0
               ;;
       esac
   }
   ```
  </action>
  <verify>
Run: `grep -c "detect_algorithm_type\|get_reference_iterations" Algorithms/common.sh`
Expected: At least 2 matches (function definitions)

Run: `bash -n Algorithms/common.sh`
Expected: No syntax errors (exit code 0)
  </verify>
  <done>
Two helper functions exist in common.sh:
- detect_algorithm_type returns "BruteForce", "DLX", "CP", or "Unknown" based on cwd
- get_reference_iterations returns correct C baseline value for algorithm/matrix combo
  </done>
</task>

<task type="auto">
  <name>Task 1b: Add validation and logging functions</name>
  <files>Algorithms/common.sh</files>
  <action>
Add the remaining validation functions immediately after the functions from Task 1a, in the same VALIDATION FUNCTIONS section:

3. **write_validation_failure()** - Log failure to benchmark_issues.json with systematic severity
   - Takes: failure_type, matrix_num, error_message, actual_iterations (optional), expected_iterations (optional)
   - Severity categorization logic:
     - CRITICAL: failure_type=="solution_invalid" OR (failure_type=="iteration_mismatch" AND abs(actual-expected) > 1)
     - WARNING: failure_type=="iteration_mismatch" AND abs(actual-expected) == 1
   - Use Python to append JSON entry to benchmark_issues.json (follow existing report_env_error pattern)

   ```bash
   write_validation_failure() {
       local failure_type="$1"
       local matrix_num="$2"
       local error_message="$3"
       local actual_iterations="${4:-0}"
       local expected_iterations="${5:-0}"
       local algo_type=$(detect_algorithm_type)
       local timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)
       local issues_file="../../../benchmark_issues.json"

       # Determine severity based on failure type and iteration delta
       local severity="CRITICAL"
       if [[ "$failure_type" == "iteration_mismatch" ]]; then
           local delta=$((actual_iterations - expected_iterations))
           if [[ $delta -lt 0 ]]; then delta=$((-delta)); fi
           if [[ $delta -le 1 ]]; then
               severity="WARNING"
           fi
       fi

       python3 -c "
import json, os

issues_file = '$issues_file'
new_entry = {
    'timestamp': '$timestamp',
    'language': '$LANGUAGE',
    'algorithm': '$algo_type',
    'matrix': '$matrix_num',
    'failure_type': '$failure_type',
    'severity': '$severity',
    'message': '''$error_message''',
    'expected_iterations': $expected_iterations,
    'actual_iterations': $actual_iterations
}

data = []
if os.path.exists(issues_file):
    try:
        with open(issues_file, 'r') as f:
            content = f.read().strip()
            if content:
                data = json.loads(content)
    except Exception:
        data = []

if not isinstance(data, list): data = []
data.append(new_entry)

with open(issues_file, 'w') as f:
    json.dump(data, f, indent=2)
"
   }
   ```

4. **validate_iteration_count()** - Check iterations against reference
   - Takes actual_iterations and matrix_num as arguments
   - BruteForce: requires exact match (return 1 if mismatch)
   - DLX/CP: allow +/-1 tolerance (return 0 if within tolerance)
   - Return 0 on pass, 1 on fail (does NOT call write_validation_failure - caller handles logging)

   ```bash
   validate_iteration_count() {
       local actual="$1"
       local matrix_num="$2"
       local expected=$(get_reference_iterations "$matrix_num")
       local algo_type=$(detect_algorithm_type)

       # Skip validation if no reference available
       if [[ $expected -eq 0 ]]; then
           return 0
       fi

       local delta=$((actual - expected))
       if [[ $delta -lt 0 ]]; then delta=$((-delta)); fi

       # BruteForce requires exact match
       if [[ "$algo_type" == "BruteForce" ]]; then
           if [[ $actual -ne $expected ]]; then
               return 1
           fi
       else
           # DLX/CP allow +/-1 tolerance
           if [[ $delta -gt 1 ]]; then
               return 1
           fi
       fi

       return 0
   }
   ```

5. **validate_solution()** - Check solution satisfies Sudoku constraints
   - Takes solver output (full stdout) and matrix_num as arguments
   - Extract the LAST "Puzzle:" section using: `echo "$output" | grep -A9 "^Puzzle:" | tail -9`
     This finds all "Puzzle:" matches with 9 lines after, then takes the last 9 lines (the solved puzzle)
   - Use inline Python to validate:
     - Grid is exactly 9x9
     - All values are 1-9
     - No duplicate in any row
     - No duplicate in any column
     - No duplicate in any 3x3 box
   - Return 0 on pass, 1 on fail

   ```bash
   validate_solution() {
       local output="$1"
       local matrix_num="$2"

       # Extract the last puzzle block (9 lines after final "Puzzle:" heading)
       local solution=$(echo "$output" | grep -A9 "^Puzzle:" | tail -9)

       if [[ -z "$solution" ]]; then
           echo "ERROR: Could not extract solution from output" >&2
           return 1
       fi

       # Validate using Python
       python3 -c "
import sys

solution = '''$solution'''
lines = [l.strip() for l in solution.strip().split('\n') if l.strip()]

if len(lines) != 9:
    print(f'Invalid grid: expected 9 rows, got {len(lines)}', file=sys.stderr)
    sys.exit(1)

grid = []
for i, line in enumerate(lines):
    # Handle both space-separated and non-separated formats
    if ' ' in line:
        cells = line.split()
    else:
        cells = list(line)

    if len(cells) != 9:
        print(f'Invalid row {i+1}: expected 9 cells, got {len(cells)}', file=sys.stderr)
        sys.exit(1)

    row = []
    for j, c in enumerate(cells):
        try:
            val = int(c)
            if val < 1 or val > 9:
                print(f'Invalid value at ({i+1},{j+1}): {val}', file=sys.stderr)
                sys.exit(1)
            row.append(val)
        except ValueError:
            print(f'Non-numeric value at ({i+1},{j+1}): {c}', file=sys.stderr)
            sys.exit(1)
    grid.append(row)

# Check rows
for i, row in enumerate(grid):
    if len(set(row)) != 9:
        print(f'Duplicate in row {i+1}', file=sys.stderr)
        sys.exit(1)

# Check columns
for j in range(9):
    col = [grid[i][j] for i in range(9)]
    if len(set(col)) != 9:
        print(f'Duplicate in column {j+1}', file=sys.stderr)
        sys.exit(1)

# Check 3x3 boxes
for box_row in range(3):
    for box_col in range(3):
        box = []
        for i in range(3):
            for j in range(3):
                box.append(grid[box_row*3 + i][box_col*3 + j])
        if len(set(box)) != 9:
            print(f'Duplicate in box ({box_row+1},{box_col+1})', file=sys.stderr)
            sys.exit(1)

print('Solution valid', file=sys.stderr)
sys.exit(0)
"
       return $?
   }
   ```
  </action>
  <verify>
Run: `grep -c "validate_iteration_count\|validate_solution\|write_validation_failure" Algorithms/common.sh`
Expected: At least 3 matches (function definitions)

Run: `grep "severity" Algorithms/common.sh | grep -c "CRITICAL\|WARNING"`
Expected: At least 2 matches (severity categorization logic)

Run: `bash -n Algorithms/common.sh`
Expected: No syntax errors (exit code 0)
  </verify>
  <done>
Three validation functions exist in common.sh:
- validate_iteration_count enforces exact match for BruteForce, +/-1 for DLX/CP
- validate_solution uses Python to extract last puzzle block via `grep -A9 "^Puzzle:" | tail -9` and check Sudoku constraints
- write_validation_failure appends JSON to benchmark_issues.json with systematic severity (CRITICAL for solution_invalid or >1 delta, WARNING for +/-1 delta)
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate validation into run_matrix()</name>
  <files>Algorithms/common.sh</files>
  <action>
Modify the run_matrix() function to call validation AFTER extracting iterations (line 246) and BEFORE determining status (line 238).

The current code flow is:
```
line 235: local output=$(cat "$temp_output")
line 238: local status="success"  # Determine status
...
line 246: local iterations=$(extract_iterations "$output")
```

Insert validation calls BETWEEN extract_iterations (line 246) and the "Default values if parsing failed" section (line 248). The validation must run AFTER extract_iterations and BEFORE status is finalized.

Insert this block after `local iterations=$(extract_iterations "$output")`:

```bash
    # VALIDATION: Check solution and iteration count (fail-fast)
    # Validate solution correctness first (more fundamental check)
    if ! validate_solution "$output" "$matrix_num"; then
        write_validation_failure "solution_invalid" "$matrix_num" \
            "Solution does not satisfy Sudoku constraints" "0" "0"
        rm -f "$temp_output" "$temp_timing"
        exit 1
    fi

    # Validate iteration count against C reference
    local expected_iterations=$(get_reference_iterations "$matrix_num")
    if [[ $expected_iterations -ne 0 ]]; then
        if ! validate_iteration_count "$iterations" "$matrix_num"; then
            write_validation_failure "iteration_mismatch" "$matrix_num" \
                "Expected $expected_iterations iterations, got $iterations" "$iterations" "$expected_iterations"
            rm -f "$temp_output" "$temp_timing"
            exit 1
        fi
    fi
```

Key implementation notes:
- Clean up temp files before exit 1 (avoid orphan temp files)
- Skip iteration validation if expected_iterations is 0 (unknown algorithm/matrix)
- Solution validation runs first (more fundamental correctness check)
- Use double-bracket [[ ]] for bash arithmetic comparison
  </action>
  <verify>
Run: `grep -B2 -A2 "validate_solution" Algorithms/common.sh | head -20`
Expected: Shows validate_solution call appearing AFTER extract_iterations context

Run: `grep -A5 "extract_iterations.*output" Algorithms/common.sh | grep -c "validate"`
Expected: At least 1 match (validation call after extract_iterations)

Run: `grep "exit 1" Algorithms/common.sh | wc -l`
Expected: At least 2 new exit points for validation failures (plus existing ones)

Run: `grep -c "rm -f.*temp_output.*temp_timing" Algorithms/common.sh`
Expected: At least 2 (cleanup before validation exits)
  </verify>
  <done>
run_matrix() now calls validate_solution() and validate_iteration_count() after extracting solver output via extract_iterations(), exits with code 1 on validation failure, cleans up temp files before exit. Validation placement verified to be between iteration extraction and status determination.
  </done>
</task>

<task type="auto">
  <name>Task 3: Test validation with C reference implementation</name>
  <files>Algorithms/BruteForce/C/metrics.json, benchmark_issues.json</files>
  <action>
Run the C reference implementation to verify validation passes:

1. Clear benchmark_issues.json:
   ```bash
   echo "[]" > benchmark_issues.json
   ```

2. Run C solver on matrix 1 (reference: 656 iterations):
   ```bash
   cd Algorithms/BruteForce/C && FORCE_COMPILE=1 ./runMe.sh ../../../Matrices/1.matrix
   ```

3. Verify validation actually executed (not just that benchmark passed):
   - Check for validation debug output: `grep -i "Solution valid" /dev/stderr` or observe stderr
   - Check exit code was 0
   - Check metrics.json was updated with matrix 1 result
   - Check benchmark_issues.json is still empty [] (no failures)

4. Verify iteration count in output:
   ```bash
   grep '"iterations": 656' Algorithms/BruteForce/C/metrics.json
   ```

5. Verify validation functions are being called by checking common.sh was sourced:
   ```bash
   grep -q "validate_solution" Algorithms/common.sh && echo "Validation functions present"
   ```

6. If validation fails for C reference (it shouldn't):
   - Check benchmark_issues.json for error details
   - Debug validate_solution or validate_iteration_count
   - C reference defines the correct behavior, so validation must pass

Optional: Run on additional matrix to confirm multi-matrix handling:
   ```bash
   cd Algorithms/BruteForce/C && ./runMe.sh ../../../Matrices/2.matrix
   ```
   Expected: 439269 iterations, validation passes
  </action>
  <verify>
Run: `cat benchmark_issues.json`
Expected: `[]` (empty array, no validation failures)

Run: `grep '"iterations": 656' Algorithms/BruteForce/C/metrics.json`
Expected: Match found (matrix 1 result with correct iterations)

Run: `grep -c "validate_solution\|validate_iteration_count" Algorithms/common.sh`
Expected: At least 4 matches (definitions plus calls in run_matrix)
  </verify>
  <done>
C reference implementation passes validation:
- validate_solution() confirms solved puzzle satisfies Sudoku constraints (verified by function call in run_matrix)
- validate_iteration_count() confirms 656 iterations matches reference
- benchmark_issues.json remains empty (no failures logged)
- metrics.json updated with successful benchmark result
- Validation functions confirmed present and integrated in common.sh
  </done>
</task>

</tasks>

<verification>
After all tasks complete, verify the complete validation pipeline:

1. **Syntax check:** `bash -n Algorithms/common.sh` returns 0
2. **Function existence:** All 5 validation functions defined in common.sh
3. **Integration:** run_matrix() calls validation functions after extract_iterations
4. **Severity logic:** write_validation_failure contains CRITICAL/WARNING categorization
5. **Solution parsing:** validate_solution uses `grep -A9 "^Puzzle:" | tail -9` pattern
6. **C reference passes:** BruteForce/C on matrix 1 produces 656 iterations, no failures logged
7. **Fail-fast works:** Exit 1 with temp file cleanup on validation failure

Run complete verification:
```bash
# Syntax check
bash -n Algorithms/common.sh && echo "SYNTAX OK"

# Function count (should be 5 new validation functions)
grep -E "^(detect_algorithm_type|get_reference_iterations|validate_iteration_count|validate_solution|write_validation_failure)\(\)" Algorithms/common.sh | wc -l

# Validation integration (calls in run_matrix)
grep -A20 "extract_iterations.*output" Algorithms/common.sh | grep -c "validate_"

# Severity categorization
grep -A5 "severity=" Algorithms/common.sh | grep -E "CRITICAL|WARNING"

# Solution extraction pattern
grep "grep -A9.*Puzzle.*tail -9" Algorithms/common.sh

# C reference test
echo "[]" > benchmark_issues.json
cd Algorithms/BruteForce/C && FORCE_COMPILE=1 ./runMe.sh ../../../Matrices/1.matrix
cat ../../../benchmark_issues.json  # Should be []
grep '"iterations": 656' metrics.json && echo "ITERATIONS OK"
```
</verification>

<success_criteria>
1. common.sh contains 5 new validation functions (detect_algorithm_type, get_reference_iterations, validate_iteration_count, validate_solution, write_validation_failure)
2. run_matrix() calls validation after extract_iterations, before metrics write
3. Validation failure exits with code 1 and cleans up temp files (fail-fast pattern)
4. C reference implementation (BruteForce/C) passes validation on all matrices
5. benchmark_issues.json logs any validation failures with timestamp, language, algorithm, matrix, severity
6. Severity is systematically categorized: CRITICAL (solution_invalid or >1 iteration delta), WARNING (+/-1 delta)
7. Solution extraction uses concrete parsing: `grep -A9 "^Puzzle:" | tail -9`
8. No changes required to any runMe.sh scripts (transparent integration)
</success_criteria>

<output>
After completion, create `.planning/phases/04-validation-infrastructure/04-01-SUMMARY.md`
</output>
