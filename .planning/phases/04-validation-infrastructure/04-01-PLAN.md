---
phase: 04-validation-infrastructure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - Algorithms/common.sh
autonomous: true

must_haves:
  truths:
    - "Benchmark run validates iteration count against C reference during execution"
    - "Benchmark run validates solution satisfies Sudoku constraints during execution"
    - "Validation failure stops benchmark immediately with error code"
    - "Validation failures are logged to benchmark_issues.json"
    - "BruteForce requires exact iteration match; DLX/CP allow +/-1 tolerance"
  artifacts:
    - path: "Algorithms/common.sh"
      provides: "Validation functions and run_matrix integration"
      exports: ["detect_algorithm_type", "get_reference_iterations", "validate_iteration_count", "validate_solution", "write_validation_failure"]
  key_links:
    - from: "Algorithms/common.sh:run_matrix()"
      to: "validate_solution() and validate_iteration_count()"
      via: "function calls after output extraction, before metrics write"
      pattern: "validate_solution.*validate_iteration_count"
    - from: "Algorithms/common.sh:validate_iteration_count()"
      to: "get_reference_iterations()"
      via: "reference lookup by algorithm type and matrix"
      pattern: "get_reference_iterations"
    - from: "Algorithms/common.sh:write_validation_failure()"
      to: "benchmark_issues.json"
      via: "Python JSON append"
      pattern: "benchmark_issues\\.json"
---

<objective>
Add validation infrastructure to common.sh that executes during benchmark runs, validating iteration counts against C reference baselines and solution correctness against Sudoku constraints, with fail-fast error handling and benchmark_issues.json logging.

Purpose: Establish algorithmic correctness foundation so invalid implementations are caught immediately during benchmark execution, not discovered later during report analysis.

Output: Modified common.sh with validation functions integrated into run_matrix(), benchmark_issues.json populated with any validation failures.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase-specific context
@.planning/phases/04-validation-infrastructure/04-CONTEXT.md
@.planning/phases/04-validation-infrastructure/04-RESEARCH.md

# Target file
@Algorithms/common.sh

# Reference data
@Metrics/C_Baselines.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add validation functions to common.sh</name>
  <files>Algorithms/common.sh</files>
  <action>
Add the following functions to common.sh AFTER the OUTPUT PARSING section (after line 119) and BEFORE the MATRIX EXECUTION section:

1. **detect_algorithm_type()** - Parse current working directory to determine algorithm type
   - Return "BruteForce" if path contains "/Algorithms/BruteForce/"
   - Return "DLX" if path contains "/Algorithms/DLX/"
   - Return "CP" if path contains "/Algorithms/CP/"
   - Return "Unknown" otherwise

2. **get_reference_iterations()** - Look up C reference iteration count
   - Takes matrix_num as argument
   - Uses detect_algorithm_type() to determine baseline set
   - Returns iteration count from C_Baselines.ts values (hardcoded in bash case statement)
   - Returns 0 if unknown algorithm/matrix (skip validation)

   Reference values from C_Baselines.ts:
   - BruteForce: 1=656, 2=439269, 3=98847, 4=9085, 5=445778, 6=622577597
   - DLX: 1=43, 2=111, 3=131, 4=70, 5=1472, 6=65
   - CP: 1=67, 2=87180, 3=4241, 4=1787, 5=31430, 6=69497705

3. **validate_iteration_count()** - Check iterations against reference
   - Takes actual_iterations and matrix_num as arguments
   - BruteForce: requires exact match (return 1 if mismatch)
   - DLX/CP: allow +/-1 tolerance (return 0 if within tolerance)
   - Log +/-1 mismatches as WARNING (call write_validation_failure with severity WARNING)
   - Return 0 on pass, 1 on fail

4. **validate_solution()** - Check solution satisfies Sudoku constraints
   - Takes solver output and matrix_num as arguments
   - Extract the LAST "Puzzle:" section (9 lines after last "Puzzle:" heading) - this is the solved puzzle
   - Use inline Python to validate:
     - Grid is exactly 9x9
     - All values are 1-9
     - No duplicate in any row
     - No duplicate in any column
     - No duplicate in any 3x3 box
   - Return 0 on pass, 1 on fail

5. **write_validation_failure()** - Log failure to benchmark_issues.json
   - Takes failure_type, matrix_num, error_message, actual_iterations (optional), expected_iterations (optional)
   - Determine severity: "WARNING" for +/-1 iteration mismatch, "CRITICAL" for solution_invalid or larger mismatch
   - Use Python to append JSON entry to benchmark_issues.json (follow existing report_env_error pattern)
   - Entry format:
     ```json
     {
       "timestamp": "ISO8601",
       "language": "$LANGUAGE",
       "algorithm": "BruteForce|DLX|CP",
       "matrix": "1-6",
       "failure_type": "iteration_mismatch|solution_invalid",
       "severity": "WARNING|CRITICAL",
       "message": "descriptive error",
       "expected_iterations": N,
       "actual_iterations": N
     }
     ```

Add a new section header comment: `# VALIDATION FUNCTIONS`
  </action>
  <verify>
Run: `grep -c "detect_algorithm_type\|get_reference_iterations\|validate_iteration_count\|validate_solution\|write_validation_failure" Algorithms/common.sh`
Expected: Should show 5 or more matches (function definitions plus calls)

Run: `bash -n Algorithms/common.sh`
Expected: No syntax errors (exit code 0)
  </verify>
  <done>
All five validation functions exist in common.sh with correct implementations:
- detect_algorithm_type returns correct algorithm based on cwd
- get_reference_iterations returns correct C baseline values
- validate_iteration_count enforces exact match for BruteForce, +/-1 for DLX/CP
- validate_solution uses Python to check Sudoku constraints
- write_validation_failure appends JSON to benchmark_issues.json
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate validation into run_matrix()</name>
  <files>Algorithms/common.sh</files>
  <action>
Modify the run_matrix() function to call validation AFTER extracting iterations (line 246) and BEFORE determining status (line 238).

Insert validation calls between the iteration extraction and status determination:

```bash
# Extract iterations from output
local iterations=$(extract_iterations "$output")

# NEW: Validate solution correctness (fail-fast)
if ! validate_solution "$output" "$matrix_num"; then
    write_validation_failure "solution_invalid" "$matrix_num" \
        "Solution does not satisfy Sudoku constraints" "0" "0"
    rm -f "$temp_output" "$temp_timing"
    exit 1
fi

# NEW: Validate iteration count against C reference (fail-fast)
local expected_iterations=$(get_reference_iterations "$matrix_num")
if [[ $expected_iterations -ne 0 ]] && ! validate_iteration_count "$iterations" "$matrix_num"; then
    write_validation_failure "iteration_mismatch" "$matrix_num" \
        "Expected $expected_iterations iterations, got $iterations" "$iterations" "$expected_iterations"
    rm -f "$temp_output" "$temp_timing"
    exit 1
fi

# Determine status (existing code continues)
local status="success"
```

Key implementation notes:
- Clean up temp files before exit 1 (avoid orphan temp files)
- Skip iteration validation if expected_iterations is 0 (unknown algorithm/matrix)
- Solution validation runs first (more fundamental correctness check)
- Use double-bracket [[ ]] for bash arithmetic comparison
  </action>
  <verify>
Run: `grep -A5 "extract_iterations" Algorithms/common.sh | grep -c "validate"`
Expected: At least 1 match (validation call after extract_iterations)

Run: `grep "exit 1" Algorithms/common.sh | wc -l`
Expected: At least 2 new exit points for validation failures
  </verify>
  <done>
run_matrix() now calls validate_solution() and validate_iteration_count() after extracting solver output, exits with code 1 on validation failure, cleans up temp files before exit.
  </done>
</task>

<task type="auto">
  <name>Task 3: Test validation with C reference implementation</name>
  <files>Algorithms/BruteForce/C/metrics.json, benchmark_issues.json</files>
  <action>
Run the C reference implementation to verify validation passes:

1. Clear benchmark_issues.json:
   ```bash
   echo "[]" > benchmark_issues.json
   ```

2. Run C solver on matrix 1 (reference: 656 iterations):
   ```bash
   cd Algorithms/BruteForce/C && ./runMe.sh ../../../Matrices/1.matrix
   ```

3. Verify validation passed:
   - Check exit code was 0
   - Check metrics.json was updated with matrix 1 result
   - Check benchmark_issues.json is still empty [] (no failures)

4. Verify iteration count in output:
   ```bash
   grep "iterations.*656" Algorithms/BruteForce/C/metrics.json
   ```

5. If validation fails for C reference (it shouldn't):
   - Check benchmark_issues.json for error details
   - Debug validate_solution or validate_iteration_count
   - C reference defines the correct behavior, so validation must pass

Optional: Run on additional matrix to confirm multi-matrix handling:
   ```bash
   cd Algorithms/BruteForce/C && ./runMe.sh ../../../Matrices/2.matrix
   ```
   Expected: 439269 iterations, validation passes
  </action>
  <verify>
Run: `cat benchmark_issues.json`
Expected: `[]` (empty array, no validation failures)

Run: `grep '"iterations": 656' Algorithms/BruteForce/C/metrics.json`
Expected: Match found (matrix 1 result with correct iterations)
  </verify>
  <done>
C reference implementation passes validation:
- validate_solution() confirms solved puzzle satisfies Sudoku constraints
- validate_iteration_count() confirms 656 iterations matches reference
- benchmark_issues.json remains empty (no failures logged)
- metrics.json updated with successful benchmark result
  </done>
</task>

</tasks>

<verification>
After all tasks complete, verify the complete validation pipeline:

1. **Syntax check:** `bash -n Algorithms/common.sh` returns 0
2. **Function existence:** All 5 validation functions defined in common.sh
3. **Integration:** run_matrix() calls validation functions
4. **C reference passes:** BruteForce/C on matrix 1 produces 656 iterations, no failures logged
5. **Fail-fast works:** Intentionally corrupt output would trigger exit 1 (optional negative test)

Run complete verification:
```bash
# Syntax check
bash -n Algorithms/common.sh && echo "SYNTAX OK"

# Function count
grep -c "^[a-z_]*() {" Algorithms/common.sh

# Validation integration
grep -c "validate_" Algorithms/common.sh

# C reference test
echo "[]" > benchmark_issues.json
cd Algorithms/BruteForce/C && FORCE_COMPILE=1 ./runMe.sh ../../../Matrices/1.matrix
cat ../../../benchmark_issues.json  # Should be []
grep '"iterations": 656' metrics.json && echo "ITERATIONS OK"
```
</verification>

<success_criteria>
1. common.sh contains 5 new validation functions (detect_algorithm_type, get_reference_iterations, validate_iteration_count, validate_solution, write_validation_failure)
2. run_matrix() calls validation after output extraction, before metrics write
3. Validation failure exits with code 1 (fail-fast pattern)
4. C reference implementation (BruteForce/C) passes validation on all matrices
5. benchmark_issues.json logs any validation failures with timestamp, language, algorithm, matrix, severity
6. No changes required to any runMe.sh scripts (transparent integration)
</success_criteria>

<output>
After completion, create `.planning/phases/04-validation-infrastructure/04-01-SUMMARY.md`
</output>
